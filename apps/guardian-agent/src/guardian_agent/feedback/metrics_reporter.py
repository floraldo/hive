"""Guardian performance metrics reporter.

This module generates weekly performance reports based on feedback data, providing
insights into Guardian's review quality and areas for improvement.
"""

from __future__ import annotations

from datetime import datetime, timedelta
from pathlib import Path

from guardian_agent.feedback.tracker import FeedbackTracker
from hive_logging import get_logger

logger = get_logger(__name__)


class MetricsReporter:
    """Generate performance reports from Guardian feedback data."""

    def __init__(self, feedback_tracker: FeedbackTracker):
        """Initialize metrics reporter.

        Args:
            feedback_tracker: FeedbackTracker instance with collected data
        """
        self.tracker = feedback_tracker

    def generate_weekly_report(self, output_path: Path | str) -> str:
        """Generate weekly performance report in markdown format.

        Args:
            output_path: Path to write markdown report

        Returns:
            Report content as string
        """
        output_path = Path(output_path)
        one_week_ago = datetime.now() - timedelta(days=7)

        # Get metrics for past week
        metrics = self.tracker.calculate_metrics(since=one_week_ago)
        all_time_metrics = self.tracker.calculate_metrics()

        # Generate report
        report = f"""# Guardian AI Performance Report
**Generated**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Period**: Last 7 days

## Executive Summary

Guardian AI has processed **{metrics["total_feedback"]}** code review comments this week.

### Key Metrics (Past 7 Days)
- **Precision**: {metrics["precision"]:.1%} (useful / [useful + not useful])
- **Acceptance Rate**: {metrics["acceptance_rate"]:.1%} (useful / total)
- **Clarity Rate**: {metrics["clarity_rate"]:.1%} (clear / total)

### Feedback Breakdown
- üëç Useful: **{metrics["useful_count"]}**
- üëé Not Useful: **{metrics["not_useful_count"]}**
- ü§î Unclear: **{metrics["unclear_count"]}**

## All-Time Performance

- **Total Reviews**: {all_time_metrics["total_feedback"]}
- **Precision**: {all_time_metrics["precision"]:.1%}
- **Acceptance Rate**: {all_time_metrics["acceptance_rate"]:.1%}
- **Clarity Rate**: {all_time_metrics["clarity_rate"]:.1%}

## Trend Analysis

{"‚úÖ **EXCELLENT**: Precision ‚â• 80%" if metrics["precision"] >= 0.80 else ""}
{"‚ö†Ô∏è  **NEEDS IMPROVEMENT**: Precision < 80%, review false positives" if 0.60 <= metrics["precision"] < 0.80 else ""}
{"üö® **CRITICAL**: Precision < 60%, model requires tuning" if metrics["precision"] < 0.60 else ""}

{"‚úÖ **HIGH CLARITY**: Clear communication ‚â• 85%" if metrics["clarity_rate"] >= 0.85 else ""}
{"‚ö†Ô∏è  **CLARITY ISSUE**: Too many unclear comments, improve phrasing" if metrics["clarity_rate"] < 0.85 else ""}

## Recommendations

### For Guardian Improvement
"""

        # Add recommendations based on metrics
        if metrics["precision"] < 0.70:
            report += """
1. **Reduce False Positives**: Review RAG context quality and golden rule thresholds
2. **Improve Pattern Detection**: Analyze "not useful" comments for pattern gaps
"""

        if metrics["clarity_rate"] < 0.80:
            report += """
3. **Enhance Comment Clarity**: Review "unclear" feedback for communication improvements
4. **Add Context**: Provide more code examples in comments
"""

        if metrics["acceptance_rate"] > 0.75:
            report += """
5. **Success Pattern**: Current approach is working well, maintain quality
"""

        report += """

### Next Steps
- Analyze false positive patterns in "not useful" feedback
- Review unclear comments for communication improvements
- Consider confidence threshold adjustments if precision < 70%

---
*Generated by Guardian AI Metrics Reporter*
"""

        # Write to file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(report, encoding="utf-8")

        logger.info(f"Weekly report generated: {output_path}")
        return report


def main() -> None:
    """CLI entrypoint for generating weekly reports."""
    tracker = FeedbackTracker()
    reporter = MetricsReporter(tracker)

    output_path = Path("claudedocs/guardian_performance_weekly.md")
    report = reporter.generate_weekly_report(output_path)

    print(report)
    print(f"\nReport saved to: {output_path}")


if __name__ == "__main__":
    main()
